{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP-2 模型測試 Notebook\n",
    "\n",
    "這個 notebook 用於測試和探索 Hugging Face 的 BLIP-2 模型功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: cuda:2\n",
      "PyTorch 版本: 2.9.1+cu126\n",
      "CUDA 可用: True\n",
      "CUDA 版本: 12.6\n",
      "GPU 名稱: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import Blip2Processor, Blip2Model\n",
    "# 配置\n",
    "MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "CACHE_DIR = \"/data/feihong/hf_cache\"\n",
    "DEVICE = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"使用設備: {DEVICE}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "    print(f\"GPU 名稱: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 測試文本處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 測試 Q-Former 的使用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([1, 7, 50304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/feihong/miniconda3/lib/python3.13/site-packages/transformers/models/blip_2/modeling_blip_2.py:1205: UserWarning: Deprecation notice: In Transformers v4.59, the default return value of `get_text_features` will change. Currently, this method returns a model output object, but starting in v4.59, it will return a tensor instead. To opt in to the new behavior now, set `legacy_output=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processor = Blip2Processor.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "# USE Blip2Model instead of Blip2ForConditionalGeneration\n",
    "model = Blip2Model.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, cache_dir=CACHE_DIR).to(DEVICE)\n",
    "\n",
    "text = [\"a giant panda eating bamboo\"]\n",
    "inputs = processor(text=text, return_tensors=\"pt\", padding=True).to(DEVICE, torch.float16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # This works in Blip2Model\n",
    "    text_features = model.get_text_features(**inputs)\n",
    "\n",
    "print(f\"Features shape: {text_features.logits.shape}\") # Expected: [1, 768]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 測試完整的圖像-文本處理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建一個測試圖像（如果沒有真實圖像）\n",
    "test_image_path = \"/data/feihong/drone_view\"  # 可以修改為實際路徑\n",
    "\n",
    "try:\n",
    "    # 嘗試加載一個測試圖像\n",
    "    if os.path.exists(test_image_path):\n",
    "        import glob\n",
    "        image_files = glob.glob(os.path.join(test_image_path, \"**/*.jpeg\"), recursive=True)\n",
    "        if image_files:\n",
    "            test_image = Image.open(image_files[0]).convert(\"RGB\")\n",
    "            print(f\"加載圖像: {image_files[0]}\")\n",
    "        else:\n",
    "            # 創建一個虛擬圖像用於測試\n",
    "            test_image = Image.new('RGB', (224, 224), color='red')\n",
    "            print(\"使用虛擬圖像進行測試\")\n",
    "    else:\n",
    "        # 創建一個虛擬圖像用於測試\n",
    "        test_image = Image.new('RGB', (224, 224), color='red')\n",
    "        print(\"使用虛擬圖像進行測試\")\n",
    "    \n",
    "    # 處理圖像和文本\n",
    "    inputs = processor(images=test_image, text=test_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    print(f\"\\n處理後的輸入:\")\n",
    "    print(f\"  - pixel_values shape: {inputs['pixel_values'].shape}\")\n",
    "    print(f\"  - input_ids shape: {inputs['input_ids'].shape}\")\n",
    "    print(f\"  - attention_mask shape: {inputs['attention_mask'].shape}\")\n",
    "    \n",
    "    # 測試完整模型前向傳播\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=50)\n",
    "        generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\n生成的文本: {generated_text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"處理圖像時出錯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 測試 Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試視覺模型\n",
    "try:\n",
    "    # 創建測試圖像\n",
    "    test_image = Image.new('RGB', (224, 224), color='blue')\n",
    "    pixel_values = processor(images=test_image, return_tensors=\"pt\")['pixel_values'].to(DEVICE)\n",
    "    \n",
    "    print(f\"輸入 pixel_values shape: {pixel_values.shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vision_outputs = model.vision_model(pixel_values)\n",
    "        print(f\"\\nVision Model 輸出:\")\n",
    "        print(f\"  - last_hidden_state shape: {vision_outputs.last_hidden_state.shape}\")\n",
    "        print(f\"  - pooler_output shape: {vision_outputs.pooler_output.shape if hasattr(vision_outputs, 'pooler_output') else 'N/A'}\")\n",
    "        \n",
    "        # 檢查特徵維度\n",
    "        B, N, D = vision_outputs.last_hidden_state.shape\n",
    "        print(f\"  - Batch size: {B}\")\n",
    "        print(f\"  - Sequence length: {N}\")\n",
    "        print(f\"  - Hidden dimension: {D}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"測試 Vision Model 時出錯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 測試 Q-Former 與文本編碼的組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嘗試使用語言模型來編碼文本，然後傳給 Q-Former\n",
    "try:\n",
    "    # 檢查語言模型\n",
    "    if hasattr(model, 'language_model'):\n",
    "        print(\"語言模型類型:\", type(model.language_model))\n",
    "        \n",
    "        # 使用語言模型編碼文本\n",
    "        with torch.no_grad():\n",
    "            # 獲取文本嵌入\n",
    "            if hasattr(model.language_model, 'get_input_embeddings'):\n",
    "                text_embeddings = model.language_model.get_input_embeddings()(input_ids)\n",
    "                print(f\"文本嵌入 shape: {text_embeddings.shape}\")\n",
    "                \n",
    "                # 如果有 query_tokens，嘗試使用 Q-Former\n",
    "                if hasattr(model.qformer, 'query_tokens'):\n",
    "                    batch_size = input_ids.shape[0]\n",
    "                    query_embeds = model.qformer.query_tokens.expand(batch_size, -1, -1).to(DEVICE)\n",
    "                    \n",
    "                    qformer_outputs = model.qformer(\n",
    "                        query_embeds=query_embeds,\n",
    "                        encoder_hidden_states=text_embeddings,\n",
    "                        encoder_attention_mask=attention_mask\n",
    "                    )\n",
    "                    print(f\"\\nQ-Former 與文本編碼結合成功!\")\n",
    "                    print(f\"  - last_hidden_state shape: {qformer_outputs.last_hidden_state.shape}\")\n",
    "                    print(f\"  - 平均池化後 shape: {qformer_outputs.last_hidden_state.mean(dim=1).shape}\")\n",
    "    else:\n",
    "        print(\"模型沒有 language_model 屬性\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"測試 Q-Former 與文本編碼時出錯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 檢查模型配置和參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型配置\n",
    "print(\"模型配置:\")\n",
    "print(f\"  - Vision model hidden size: {model.vision_model.config.hidden_size}\")\n",
    "print(f\"  - Q-Former hidden size: {model.qformer.config.hidden_size}\")\n",
    "if hasattr(model, 'language_model') and hasattr(model.language_model, 'config'):\n",
    "    print(f\"  - Language model hidden size: {model.language_model.config.hidden_size}\")\n",
    "\n",
    "# 查看 Q-Former 的詳細配置\n",
    "print(\"\\nQ-Former 配置詳情:\")\n",
    "qformer_config = model.qformer.config\n",
    "for key in ['hidden_size', 'num_attention_heads', 'num_hidden_layers', 'intermediate_size']:\n",
    "    if hasattr(qformer_config, key):\n",
    "        print(f\"  - {key}: {getattr(qformer_config, key)}\")\n",
    "\n",
    "# 檢查 Q-Former 的參數\n",
    "print(\"\\nQ-Former 參數統計:\")\n",
    "total_params = sum(p.numel() for p in model.qformer.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.qformer.parameters() if p.requires_grad)\n",
    "print(f\"  - 總參數數: {total_params:,}\")\n",
    "print(f\"  - 可訓練參數數: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 測試不同長度的文本輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試不同長度的文本\n",
    "test_texts = [\n",
    "    \"Short text\",\n",
    "    \"This is a medium length text description for testing\",\n",
    "    \"This is a very long text description that contains many words and should test the model's ability to handle longer sequences with proper padding and truncation mechanisms\"\n",
    "]\n",
    "\n",
    "print(\"測試不同長度的文本輸入:\\n\")\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"文本 {i+1} (長度: {len(text.split())} 詞): {text}\")\n",
    "    \n",
    "    # 不填充，保持原始長度\n",
    "    text_inputs = processor(text=text, return_tensors=\"pt\", padding=False, truncation=True, max_length=128)\n",
    "    input_ids = text_inputs['input_ids'].to(DEVICE)\n",
    "    attention_mask = text_inputs['attention_mask'].to(DEVICE)\n",
    "    \n",
    "    print(f\"  - input_ids shape: {input_ids.shape}\")\n",
    "    print(f\"  - attention_mask shape: {attention_mask.shape}\")\n",
    "    print(f\"  - 實際長度: {input_ids.shape[1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 測試 Q-Former 的正確使用方式（針對文本編碼）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根據前面的測試，確定 Q-Former 的正確使用方式\n",
    "# 這對於修復 train_blip2.py 中的 text_forward 方法很重要\n",
    "\n",
    "test_text = \"A beautiful landscape with mountains and trees\"\n",
    "text_inputs = processor(text=test_text, return_tensors=\"pt\", padding=False, truncation=True, max_length=128)\n",
    "input_ids = text_inputs['input_ids'].to(DEVICE)\n",
    "attention_mask = text_inputs['attention_mask'].to(DEVICE)\n",
    "\n",
    "print(\"測試 Q-Former 用於文本編碼的不同方法:\\n\")\n",
    "\n",
    "# 方法 1: 使用語言模型獲取文本嵌入，然後傳給 Q-Former\n",
    "if hasattr(model, 'language_model') and hasattr(model.language_model, 'get_input_embeddings'):\n",
    "    print(\"方法 1: 使用語言模型的輸入嵌入\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = model.language_model.get_input_embeddings()(input_ids)\n",
    "            \n",
    "            if hasattr(model.qformer, 'query_tokens'):\n",
    "                batch_size = input_ids.shape[0]\n",
    "                query_embeds = model.qformer.query_tokens.expand(batch_size, -1, -1).to(DEVICE)\n",
    "                \n",
    "                qformer_outputs = model.qformer(\n",
    "                    query_embeds=query_embeds,\n",
    "                    encoder_hidden_states=text_embeddings,\n",
    "                    encoder_attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                text_features = qformer_outputs.last_hidden_state.mean(dim=1)\n",
    "                print(f\"  成功! 輸出特徵 shape: {text_features.shape}\")\n",
    "                print(f\"  這是在 train_blip2.py 中應該使用的方法\")\n",
    "    except Exception as e:\n",
    "        print(f\"  失敗: {e}\")\n",
    "\n",
    "print(\"\\n總結:\")\n",
    "print(\"Q-Former 需要 query_embeds 參數，可以從 model.qformer.query_tokens 獲取\")\n",
    "print(\"文本應該先通過語言模型的嵌入層轉換為 embeddings，然後作為 encoder_hidden_states 傳入\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
