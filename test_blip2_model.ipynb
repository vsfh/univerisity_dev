{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP-2 模型測試 Notebook\n",
    "\n",
    "這個 notebook 用於測試和探索 Hugging Face 的 BLIP-2 模型功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: cuda:2\n",
      "PyTorch 版本: 2.9.1+cu128\n",
      "CUDA 可用: True\n",
      "CUDA 版本: 12.8\n",
      "GPU 名稱: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "# 配置\n",
    "MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "CACHE_DIR = \"/data/feihong/hf_cache\"\n",
    "DEVICE = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"使用設備: {DEVICE}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "    print(f\"GPU 名稱: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加載模型和處理器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加載處理器...\n",
      "處理器加載完成！\n",
      "加載模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.66s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 2 has a total capacity of 23.56 GiB of which 21.25 MiB is free. Process 2655022 has 17.07 GiB memory in use. Including non-PyTorch memory, this process has 6.44 GiB memory in use. Of the allocated memory 5.89 GiB is allocated by PyTorch, and 305.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m加載模型...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m model = Blip2ForConditionalGeneration.from_pretrained(\n\u001b[32m      9\u001b[39m     MODEL_NAME,\n\u001b[32m     10\u001b[39m     torch_dtype=torch.float16 \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(DEVICE) \u001b[38;5;28;01melse\u001b[39;00m torch.float32,\n\u001b[32m     11\u001b[39m     cache_dir=CACHE_DIR\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m model.eval()\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m模型加載完成！\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/feihong/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py:4343\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4340\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4341\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4342\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/feihong/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/feihong/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/feihong/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 930 (4 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/feihong/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/feihong/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/feihong/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 2 has a total capacity of 23.56 GiB of which 21.25 MiB is free. Process 2655022 has 17.07 GiB memory in use. Including non-PyTorch memory, this process has 6.44 GiB memory in use. Of the allocated memory 5.89 GiB is allocated by PyTorch, and 305.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 加載處理器\n",
    "print(\"加載處理器...\")\n",
    "processor = Blip2Processor.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "print(\"處理器加載完成！\")\n",
    "\n",
    "# 加載模型\n",
    "print(\"加載模型...\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if \"cuda\" in str(DEVICE) else torch.float32,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"模型加載完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 探索模型結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型的主要組件\n",
    "print(\"模型組件:\")\n",
    "print(f\"  - vision_model: {type(model.vision_model)}\")\n",
    "print(f\"  - qformer: {type(model.qformer)}\")\n",
    "print(f\"  - language_model: {type(model.language_model)}\")\n",
    "\n",
    "# 查看 Q-Former 的結構\n",
    "print(\"\\nQ-Former 結構:\")\n",
    "print(f\"  - 類型: {type(model.qformer)}\")\n",
    "print(f\"  - 配置: {model.qformer.config}\")\n",
    "\n",
    "# 檢查 Q-Former 是否有 query_embeds 相關屬性\n",
    "qformer_attrs = [attr for attr in dir(model.qformer) if 'query' in attr.lower() or 'embed' in attr.lower()]\n",
    "print(f\"\\nQ-Former 相關屬性: {qformer_attrs}\")\n",
    "\n",
    "# 查看 Q-Former 的 forward 方法簽名\n",
    "if hasattr(model.qformer, 'forward'):\n",
    "    sig = inspect.signature(model.qformer.forward)\n",
    "    print(f\"\\nQ-Former forward 方法參數:\")\n",
    "    for param_name, param in sig.parameters.items():\n",
    "        print(f\"  - {param_name}: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 測試文本處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試文本: A beautiful landscape with mountains and trees\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m測試文本: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 使用處理器處理文本\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m text_inputs = \u001b[43mprocessor\u001b[49m(text=test_text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m128\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m文本處理結果:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - input_ids shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "# 測試文本處理\n",
    "test_text = \"A beautiful landscape with mountains and trees\"\n",
    "print(f\"測試文本: {test_text}\")\n",
    "\n",
    "# 使用處理器處理文本\n",
    "text_inputs = processor(text=test_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "print(f\"\\n文本處理結果:\")\n",
    "print(f\"  - input_ids shape: {text_inputs['input_ids'].shape}\")\n",
    "print(f\"  - attention_mask shape: {text_inputs['attention_mask'].shape}\")\n",
    "print(f\"  - input_ids: {text_inputs['input_ids']}\")\n",
    "print(f\"  - attention_mask: {text_inputs['attention_mask']}\")\n",
    "\n",
    "# 移動到設備\n",
    "input_ids = text_inputs['input_ids'].to(DEVICE)\n",
    "attention_mask = text_inputs['attention_mask'].to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 測試 Q-Former 的使用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 Q-Former 是否有 query_tokens\n",
    "if hasattr(model.qformer, 'query_tokens'):\n",
    "    query_tokens = model.qformer.query_tokens\n",
    "    print(f\"Query tokens shape: {query_tokens.shape}\")\n",
    "    print(f\"Query tokens dtype: {query_tokens.dtype}\")\n",
    "    \n",
    "    # 擴展 query_tokens 到批次大小\n",
    "    batch_size = input_ids.shape[0]\n",
    "    query_embeds = query_tokens.expand(batch_size, -1, -1).to(DEVICE)\n",
    "    print(f\"擴展後的 query_embeds shape: {query_embeds.shape}\")\n",
    "    \n",
    "    # 嘗試使用 Q-Former（僅使用 query_embeds）\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            qformer_outputs = model.qformer(\n",
    "                query_embeds=query_embeds\n",
    "            )\n",
    "        print(f\"\\nQ-Former 輸出成功!\")\n",
    "        print(f\"  - last_hidden_state shape: {qformer_outputs.last_hidden_state.shape}\")\n",
    "        print(f\"  - pooler_output: {qformer_outputs.pooler_output if hasattr(qformer_outputs, 'pooler_output') else 'N/A'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nQ-Former 調用失敗: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Q-Former 沒有 query_tokens 屬性\")\n",
    "    print(\"可用的屬性:\", [attr for attr in dir(model.qformer) if not attr.startswith('_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 測試完整的圖像-文本處理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建一個測試圖像（如果沒有真實圖像）\n",
    "test_image_path = \"/data/feihong/drone_view\"  # 可以修改為實際路徑\n",
    "\n",
    "try:\n",
    "    # 嘗試加載一個測試圖像\n",
    "    if os.path.exists(test_image_path):\n",
    "        import glob\n",
    "        image_files = glob.glob(os.path.join(test_image_path, \"**/*.jpeg\"), recursive=True)\n",
    "        if image_files:\n",
    "            test_image = Image.open(image_files[0]).convert(\"RGB\")\n",
    "            print(f\"加載圖像: {image_files[0]}\")\n",
    "        else:\n",
    "            # 創建一個虛擬圖像用於測試\n",
    "            test_image = Image.new('RGB', (224, 224), color='red')\n",
    "            print(\"使用虛擬圖像進行測試\")\n",
    "    else:\n",
    "        # 創建一個虛擬圖像用於測試\n",
    "        test_image = Image.new('RGB', (224, 224), color='red')\n",
    "        print(\"使用虛擬圖像進行測試\")\n",
    "    \n",
    "    # 處理圖像和文本\n",
    "    inputs = processor(images=test_image, text=test_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    print(f\"\\n處理後的輸入:\")\n",
    "    print(f\"  - pixel_values shape: {inputs['pixel_values'].shape}\")\n",
    "    print(f\"  - input_ids shape: {inputs['input_ids'].shape}\")\n",
    "    print(f\"  - attention_mask shape: {inputs['attention_mask'].shape}\")\n",
    "    \n",
    "    # 測試完整模型前向傳播\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=50)\n",
    "        generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\n生成的文本: {generated_text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"處理圖像時出錯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 測試 Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試視覺模型\n",
    "try:\n",
    "    # 創建測試圖像\n",
    "    test_image = Image.new('RGB', (224, 224), color='blue')\n",
    "    pixel_values = processor(images=test_image, return_tensors=\"pt\")['pixel_values'].to(DEVICE)\n",
    "    \n",
    "    print(f\"輸入 pixel_values shape: {pixel_values.shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vision_outputs = model.vision_model(pixel_values)\n",
    "        print(f\"\\nVision Model 輸出:\")\n",
    "        print(f\"  - last_hidden_state shape: {vision_outputs.last_hidden_state.shape}\")\n",
    "        print(f\"  - pooler_output shape: {vision_outputs.pooler_output.shape if hasattr(vision_outputs, 'pooler_output') else 'N/A'}\")\n",
    "        \n",
    "        # 檢查特徵維度\n",
    "        B, N, D = vision_outputs.last_hidden_state.shape\n",
    "        print(f\"  - Batch size: {B}\")\n",
    "        print(f\"  - Sequence length: {N}\")\n",
    "        print(f\"  - Hidden dimension: {D}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"測試 Vision Model 時出錯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 測試 Q-Former 與文本編碼的組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嘗試使用語言模型來編碼文本，然後傳給 Q-Former\n",
    "try:\n",
    "    # 檢查語言模型\n",
    "    if hasattr(model, 'language_model'):\n",
    "        print(\"語言模型類型:\", type(model.language_model))\n",
    "        \n",
    "        # 使用語言模型編碼文本\n",
    "        with torch.no_grad():\n",
    "            # 獲取文本嵌入\n",
    "            if hasattr(model.language_model, 'get_input_embeddings'):\n",
    "                text_embeddings = model.language_model.get_input_embeddings()(input_ids)\n",
    "                print(f\"文本嵌入 shape: {text_embeddings.shape}\")\n",
    "                \n",
    "                # 如果有 query_tokens，嘗試使用 Q-Former\n",
    "                if hasattr(model.qformer, 'query_tokens'):\n",
    "                    batch_size = input_ids.shape[0]\n",
    "                    query_embeds = model.qformer.query_tokens.expand(batch_size, -1, -1).to(DEVICE)\n",
    "                    \n",
    "                    qformer_outputs = model.qformer(\n",
    "                        query_embeds=query_embeds,\n",
    "                        encoder_hidden_states=text_embeddings,\n",
    "                        encoder_attention_mask=attention_mask\n",
    "                    )\n",
    "                    print(f\"\\nQ-Former 與文本編碼結合成功!\")\n",
    "                    print(f\"  - last_hidden_state shape: {qformer_outputs.last_hidden_state.shape}\")\n",
    "                    print(f\"  - 平均池化後 shape: {qformer_outputs.last_hidden_state.mean(dim=1).shape}\")\n",
    "    else:\n",
    "        print(\"模型沒有 language_model 屬性\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"測試 Q-Former 與文本編碼時出錯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 檢查模型配置和參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型配置\n",
    "print(\"模型配置:\")\n",
    "print(f\"  - Vision model hidden size: {model.vision_model.config.hidden_size}\")\n",
    "print(f\"  - Q-Former hidden size: {model.qformer.config.hidden_size}\")\n",
    "if hasattr(model, 'language_model') and hasattr(model.language_model, 'config'):\n",
    "    print(f\"  - Language model hidden size: {model.language_model.config.hidden_size}\")\n",
    "\n",
    "# 查看 Q-Former 的詳細配置\n",
    "print(\"\\nQ-Former 配置詳情:\")\n",
    "qformer_config = model.qformer.config\n",
    "for key in ['hidden_size', 'num_attention_heads', 'num_hidden_layers', 'intermediate_size']:\n",
    "    if hasattr(qformer_config, key):\n",
    "        print(f\"  - {key}: {getattr(qformer_config, key)}\")\n",
    "\n",
    "# 檢查 Q-Former 的參數\n",
    "print(\"\\nQ-Former 參數統計:\")\n",
    "total_params = sum(p.numel() for p in model.qformer.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.qformer.parameters() if p.requires_grad)\n",
    "print(f\"  - 總參數數: {total_params:,}\")\n",
    "print(f\"  - 可訓練參數數: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 測試不同長度的文本輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試不同長度的文本\n",
    "test_texts = [\n",
    "    \"Short text\",\n",
    "    \"This is a medium length text description for testing\",\n",
    "    \"This is a very long text description that contains many words and should test the model's ability to handle longer sequences with proper padding and truncation mechanisms\"\n",
    "]\n",
    "\n",
    "print(\"測試不同長度的文本輸入:\\n\")\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"文本 {i+1} (長度: {len(text.split())} 詞): {text}\")\n",
    "    \n",
    "    # 不填充，保持原始長度\n",
    "    text_inputs = processor(text=text, return_tensors=\"pt\", padding=False, truncation=True, max_length=128)\n",
    "    input_ids = text_inputs['input_ids'].to(DEVICE)\n",
    "    attention_mask = text_inputs['attention_mask'].to(DEVICE)\n",
    "    \n",
    "    print(f\"  - input_ids shape: {input_ids.shape}\")\n",
    "    print(f\"  - attention_mask shape: {attention_mask.shape}\")\n",
    "    print(f\"  - 實際長度: {input_ids.shape[1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 測試 Q-Former 的正確使用方式（針對文本編碼）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根據前面的測試，確定 Q-Former 的正確使用方式\n",
    "# 這對於修復 train_blip2.py 中的 text_forward 方法很重要\n",
    "\n",
    "test_text = \"A beautiful landscape with mountains and trees\"\n",
    "text_inputs = processor(text=test_text, return_tensors=\"pt\", padding=False, truncation=True, max_length=128)\n",
    "input_ids = text_inputs['input_ids'].to(DEVICE)\n",
    "attention_mask = text_inputs['attention_mask'].to(DEVICE)\n",
    "\n",
    "print(\"測試 Q-Former 用於文本編碼的不同方法:\\n\")\n",
    "\n",
    "# 方法 1: 使用語言模型獲取文本嵌入，然後傳給 Q-Former\n",
    "if hasattr(model, 'language_model') and hasattr(model.language_model, 'get_input_embeddings'):\n",
    "    print(\"方法 1: 使用語言模型的輸入嵌入\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = model.language_model.get_input_embeddings()(input_ids)\n",
    "            \n",
    "            if hasattr(model.qformer, 'query_tokens'):\n",
    "                batch_size = input_ids.shape[0]\n",
    "                query_embeds = model.qformer.query_tokens.expand(batch_size, -1, -1).to(DEVICE)\n",
    "                \n",
    "                qformer_outputs = model.qformer(\n",
    "                    query_embeds=query_embeds,\n",
    "                    encoder_hidden_states=text_embeddings,\n",
    "                    encoder_attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                text_features = qformer_outputs.last_hidden_state.mean(dim=1)\n",
    "                print(f\"  成功! 輸出特徵 shape: {text_features.shape}\")\n",
    "                print(f\"  這是在 train_blip2.py 中應該使用的方法\")\n",
    "    except Exception as e:\n",
    "        print(f\"  失敗: {e}\")\n",
    "\n",
    "print(\"\\n總結:\")\n",
    "print(\"Q-Former 需要 query_embeds 參數，可以從 model.qformer.query_tokens 獲取\")\n",
    "print(\"文本應該先通過語言模型的嵌入層轉換為 embeddings，然後作為 encoder_hidden_states 傳入\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
